---
title: "Lab 5: Sea-Level Rise"
author: "Lucia Romero-Alston (lmr12)"
jupyter: julia-1.10
date: 2024-02-16

format: 
    html: default

    # YOU DO NOT NEED BOTH PDF AND DOCX.
    # COMMENT OR DELETE THE ONE YOU DON'T WANT TO USE.
    pdf:
        documentclass: article
        fontsize: 11pt
        geometry:
            - margin=1in  
        number-sections: true
        code-line-numbers: true
    #docx: 
    #    toc: true
    #    fig-format: png
    #    number-sections: true
    #    code-line-numbers: true

date-format: "ddd., MMM. D"
bibliography: references.bib
---

# Setup

## The usual

As always:

1. Clone the lab repository to your computer
1. Open the lab repository in VS Code
1. Open the Julia REPL and activate, then instantiate, the lab environment
1. Make sure you can render: `quarto render template.qmd` in the terminal.
    - If you run into issues, try running `] build IJulia` in the Julia REPL (`]` enters the package manager).
    - If you still have issues, try opening up `blankfile.py`. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use.


## Load packages

```{julia}
using CSV
using DataFrames
using DataFramesMeta
using Distributions
using Plots
using StatsPlots
using Unitful

Plots.default(; margin=5Plots.mm)
```

## Local package

```{julia}
using Revise
using HouseElevation
```

## House

This creates the House object which contains all of the relevant information four our building, including: depth-damage function, area, cost (USD), elevation relative to gauge, and metadata
The building I am studying is Fisherman's Wharf at 2200 Harborside Drive Galveston, TX.
I got the information for the building area from the cvent website for event space. The value of the property results from searches on Zillow for average building prices in the area. Finally, the depth-damage curve which I chose is a result of location, building type, and what I am looking to analyze for damages. 
```{julia}
house = let
    haz_fl_dept = CSV.read("data/haz_fl_dept.csv", DataFrame) # read in the file
    desc = "Cafeteria Restaurant, structure"
    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want
    area = 4004u"ft^2"
    height_above_gauge = 4*u"ft" # height is actually 3.74ft but this function only takes integer values so we will round to 4
    House(
        row;
        area=area,
        height_above_gauge=height_above_gauge,
        value_usd=400_000,
    )
end
```

We can use this House object to find a relationship between flood depth and home damage. 
```{julia}
let
    depths = uconvert.(u"ft", (-7.0u"ft"):(1.0u"inch"):(30.0u"ft"))
    damages = house.ddf.(depths) .* house.value_usd ./ 1000
    scatter(
        depths,
        damages;
        xlabel="Flood Depth",
        ylabel="Damage (Thousand USD)",
        label="$(house.description)\n($(house.source))",
        legend=:bottomright,
        size=(800, 400),
        yformatter=:plain, # prevents scientific notation
    )
end
```

Here we will plot the cost of ekevating our building from 0 to 14 ft using the elevation_cost function.
```{julia}
elevation_cost(house, 4u"ft")
```

```{julia}
let
    # elevations from 0 to 14 feet at 0.25ft differences
    elevations = 0u"ft":0.25u"ft":14u"ft" 
    # cost of elevating the house by each of these heights
    costs = [elevation_cost(house, eᵢ) for eᵢ in elevations]
    scatter(
        elevations,
        costs ./ 1_000;
        xlabel="Elevation",
        ylabel="Cost (Thousand USD)",
        label="$(house.description)\n($(house.source))",
        legend=:bottomright,
        size=(800, 400),
        yformatter=:plain, # prevents scientific notation
    )
end
```

## Sea-level Rise

Here we will model sea-level rise using the approach by Otto et al. (2017) that is calibrated to historical sea-level rise in the Netherlands.
```{julia}
slr_scenarios = let
    df = CSV.read("data/slr_oddo.csv", DataFrame)
    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]
end
println("There are $(length(slr_scenarios)) parameter sets")
```

## Storm Surge

Here we are modeling a distribution for storm surge by sampling parameteres from the range centered in the distribution from Lab 3. This helps to account for the uncertainty in storm surge. We can call this function to get different distributions for storm surge.
```{julia}
function draw_surge_distribution()
    μ = rand(Normal(5, 1))
    σ = rand(Exponential(1.5))
    ξ = rand(Normal(0.1, 0.05))
    GeneralizedExtremeValue(μ, σ, ξ)
end
```

We want a function that draws samples from the storm surge distribution.
```{julia}
function surge_dist_sample()
    surge_sample = rand(draw_surge_distribution())
end
```

## Discount Rate

The discount rate is important in NPV analysis, but there are both random and not random factors that go into discounting, which are acounted for in the following function. This function already draws a sample from a normal distribution for discount rates.
```{julia}
function draw_discount_rate()
    return rand(Normal(0.04, 0.02))
end
```

## Running the Simulation

We are adding the object ModelParams, which contains all of the parameters of the model that dont change from one simulation to the next.
Note: it may be interesting to run this simulation considering different years.
```{julia}
p = ModelParams(
    house=house,
    years=2024:2100
)
```

Here we are creating an object that will hold our state of the world (SOW).
```{julia}
sow = SOW(
    rand(slr_scenarios),
    draw_surge_distribution(),
    draw_discount_rate()
)
```

Here we are defininf out action, a. This action is to raise the building to a fixed elevation.
```{julia}
a = Action(3.0u"ft")
```

This function runs the simulation, taking in the model parameters, SOW, and the action, and returning the net present value of doing the action.
```{julia}
res = run_sim(a, sow, p)
```

## Large Ensemble

This will be a large ensemble of simulations which samples many SOWs for a range of actions (house elevations). 
```{julia}
#randomize actions and then take a random for the actions
heights = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0] .*u"ft"
sows = [SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for _ in 1:200] # for 200 SOWs
actions = [Action(rand(heights)) for _ in 1:200] # these are all the same
results = [run_sim(a, s, p) for (a, s) in zip(actions, sows)]
```

Here we create a dataframe for our results.
```{julia}
years = 2024:2100
df = DataFrame(
    npv=results,
    Δh_ft=[a.Δh_ft for a in actions],
    slr_s=[mean(s.slr.(years)) for s in sows],
    slr_a=[s.slr.a for s in sows],
    slr_b=[s.slr.b for s in sows],
    slr_c=[s.slr.c for s in sows],
    slr_tstar=[s.slr.tstar for s in sows],
    slr_cstar=[s.slr.cstar for s in sows],
    surge_val=[mean(s.surge_dist) for s in sows],
    surge_μ=[s.surge_dist.μ for s in sows],
    surge_σ=[s.surge_dist.σ for s in sows],
    surge_ξ=[s.surge_dist.ξ for s in sows],
    discount_rate=[s.discount_rate for s in sows],
)
```

It can be helpful to keep some parameters constant and vary only one at a time to wee which has the greatest evvect on the NPV.
Here we will vary
## Analysis

In order to underestand our results we can represent the data through plots.
The following is the plot if the relationship between house elevation and NPV.
```{julia}
scatter(
    df.Δh_ft,
    df.npv ./ 1_000;
    xlabel="Elevation",
    ylabel="NPV (Thousand USD)",
    label="$(house.description)\n($(house.source))",
    legend=:bottomright,
    size=(800, 400),
    yformatter=:plain, # prevents scientific notation
)
```

We can also find the relationship between the NPV and other parameters to determine which is the most important and which has the greatest influence on the NPV.
Effect of the discount rate on NPV:
```{julia}
scatter(
    df.discount_rate,
    df.npv ./ 1_000;
    xlabel="Discount Rate",
    ylabel="NPV (Thousand USD)",
    label="$(house.description)\n($(house.source))",
    legend=:bottomright,
    size=(800, 400),
    yformatter=:plain, # prevents scientific notation
)
```

Effect of sea level rise on NPV:
```{julia}
scatter(
    df.slr_s,
    df.npv ./ 1_000;
    xlabel="Mean Sea Level Rise",
    ylabel="NPV (Thousand USD)",
    label="$(house.description)\n($(house.source))",
    legend=:bottomright,
    size=(800, 400),
    yformatter=:plain, # prevents scientific notation
)
```

Effect of storm surge on NPV:
```{julia}
scatter(
    df.surge_val,
    df.npv ./ 1_000;
    xlabel="Mean Storm Surge",
    ylabel="NPV (Thousand USD)",
    label="$(house.description)\n($(house.source))",
    legend=:bottomright,
    size=(800, 400),
    yformatter=:plain, # prevents scientific notation
)
```

When do we get the best results?
The best results seem to be at around the 11-ft area of the graph. In this reigon, the scatter points demonstrate a smaller negative NPV, meaning that in the long run, we are still spending money, but less money than for many of the other cases.

When do you get the worst resutls?
The worst results appear between 0- and 2-ft. This is the region in which we find the highest lows and the highest highs. This is most likely a result of the cost of elevating the house an elevation which is not enough to make a singificant difference when it comes to flood depth and damage.

What are the most important parameters?
Using the graphs comparing the different parameters to the net present value, we can analyze which parameters have a greater effect on NPV. While it is clear that all aprameters can have drastic effects on the NPV, the one which most affects it seems to be the discount rate. This is because for the smallest variations in discount rate, there seems to be the largest differences in NPV. This is important information that can be gained from this sort of analysis because it will inform homewoners, developers and planners that they must be attentive to economic shifts and possibilities, because it is likely to have a large effect on the NPV of their project.

If you had unlimited computing power, would you run more simulations? How many?
If it were possible to have unlimited computing power, I would run trillions of simulations. This is because it would be extrememly to understand and have an exact outcome for every single situation. This would provide homeowners, developers, and policy makers with exact knowledge. This is not realistic of course, so the information we have is a great source of information into what are the more important parameters and on trends and correlations in what we are looking for,

What are the implications of your results for decision-making?
The implications of these results is that it is clear that elevating ones home results in monetary saving, but that below a certain threshold, it there is a chance that it is actually more expensive than doing nothing at all. This can be a good guide for people in regions of flood risk to understand what elevating ones home can do, but it should not be taken as law. This is because there are considerations that are not factored in, and uncertainty in many places including future economies, climate change, and flood depth among many other things.